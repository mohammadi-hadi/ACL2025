<div align="center">

# ACL 2025 Paper Website

### Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation

[![ACL 2025](https://img.shields.io/badge/ACL-2025-green.svg)](https://2025.aclweb.org/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.13138-b31b1b.svg)](https://arxiv.org/abs/2507.13138)
[![Website](https://img.shields.io/badge/Website-Live-blue.svg)](https://mohammadi-hadi.github.io/ACL2025/)

*Project website for our ACL 2025 paper on LLM annotation reliability*

[Live Website](https://mohammadi-hadi.github.io/ACL2025/) • [Paper](https://arxiv.org/abs/2507.13138)

---

</div>

## Overview

This repository hosts the project website for our paper presented at ACL 2025. The paper investigates the reliability of Large Language Model (LLM) annotations in the context of demographic bias and model explanation for sexism detection tasks.

## Paper Abstract

We assess the reliability of LLM annotations examining demographic bias and the quality of model explanations. Our study uses mixed-effects models to analyze annotation patterns across different LLMs and demographic groups.

## Keywords

- Large Language Models (LLMs)
- Demographic Bias
- Model Explanation
- NLP
- Sexism Detection
- Annotation Reliability
- Explainable AI (XAI)
- Fairness in AI
- Mixed Effects Models

## Website

The live website is available at: [https://mohammadi-hadi.github.io/ACL2025/](https://mohammadi-hadi.github.io/ACL2025/)

## Repository Structure

```
ACL2025/
├── index.html           # Main website page
├── static/
│   ├── css/            # Stylesheets
│   ├── js/             # JavaScript files
│   └── images/         # Images and figures
├── Hadi Mohammadi CV.pdf  # Author CV
└── README.md           # This file
```

## Local Development

```bash
# Clone the repository
git clone https://github.com/mohammadi-hadi/ACL2025.git
cd ACL2025

# Start a local server
python -m http.server 8000

# Visit http://localhost:8000
```

## Citation

```bibtex
@inproceedings{mohammadi2025reliability,
  title={Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation},
  author={Mohammadi, Hadi and others},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year={2025}
}
```

## Related Work

- [EvalMORAAL](https://github.com/mohammadi-hadi/EvalMORAAL) - Moral alignment evaluation framework
- [Explainable Sexism Detection](https://github.com/mohammadi-hadi/Explainable-Sexism-Detection) - Explainable ML for sexism detection

## Contact

- **Hadi Mohammadi** - Utrecht University
- **Email**: [h.mohammadi@uu.nl](mailto:h.mohammadi@uu.nl)
- **Website**: [mohammadi.cv](https://mohammadi.cv)
